import pandas as pd
import numpy as np
from pymongo import MongoClient
from bson.decimal128 import Decimal128
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import load_model, Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import os
import sys

# --------------------------------------------------------
# 1. –ö–û–ù–°–¢–ê–ù–¢–ò
# --------------------------------------------------------


MONGO_URI = "mongodb+srv://kristijanjovik_db_user:Z7ElsP3JsscmHMrj@cluster0.pkcuhbd.mongodb.net/"
DATABASE_NAME = "crypto_db"
COLLECTION_NAME = "historical_data"
MODEL_SAVE_PATH = 'model_weights.h5'

# –•–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞ –º–æ–¥–µ–ª–æ—Ç
LOOKBACK_PERIOD = 30  # –í–ª–µ–∑–Ω–∏ –¥–µ–Ω–æ–≤–∏ –∑–∞ –ø—Ä–µ–¥–≤–∏–¥—É–≤–∞—ö–µ –Ω–∞ —Å–ª–µ–¥–Ω–∏–æ—Ç –¥–µ–Ω
CLOSE_PRICE_INDEX = 3  # –ò–Ω–¥–µ–∫—Å –Ω–∞ 'close' –≤–æ –ª–∏—Å—Ç–∞—Ç–∞ features ['open', 'high', 'low', 'close', 'Volume']
TARGET_SYMBOL_ID = "bitcoin"  # –î–µ—Ñ–æ–ª—Ç–µ–Ω —Å–∏–º–±–æ–ª –∑–∞ –ø—Ä–≤–∏—á–Ω–æ —Ç—Ä–µ–Ω–∏—Ä–∞—ö–µ


# --------------------------------------------------------
# 2. –ü–û–ú–û–®–ù–ò –§–£–ù–ö–¶–ò–ò (Data Handling)
# --------------------------------------------------------

def load_data_from_mongo(target_symbol_id):
    """
    –í—á–∏—Ç—É–≤–∞ –∏—Å—Ç–æ—Ä–∏—Å–∫–∏ OHLCV –ø–æ–¥–∞—Ç–æ—Ü–∏ –æ–¥ MongoDB.
    """
    print(f"–ü–æ—á–Ω—É–≤–∞–º –≤—á–∏—Ç—É–≤–∞—ö–µ –Ω–∞ –ø–æ–¥–∞—Ç–æ—Ü–∏ –∑–∞ {target_symbol_id}...")

    try:
        # –ü–æ–≤—Ä–∑—É–≤–∞—ö–µ —Å–æ MongoDB
        client = MongoClient(MONGO_URI)
        db = client[DATABASE_NAME]
        collection = db[COLLECTION_NAME]

        # –í—á–∏—Ç—É–≤–∞—ö–µ –Ω–∞ –ø–æ–¥–∞—Ç–æ—Ü–∏
        cursor = collection.find(
            {"symbolId": target_symbol_id},
            {"_id": 0, "timestamp": 1, "open": 1, "high": 1, "low": 1, "close": 1, "totalVolume": 1}
        ).sort("timestamp", 1)

        df = pd.DataFrame(list(cursor))

        # –ó–∞—Ç–≤–æ—Ä–∞—ö–µ –Ω–∞ –∫–æ–Ω–µ–∫—Ü–∏—ò–∞—Ç–∞
        client.close()

        if df.empty:
            print(f"–ì–†–ï–®–ö–ê: –ù–µ–º–∞ –ø—Ä–æ–Ω–∞—ò–¥–µ–Ω–æ –ø–æ–¥–∞—Ç–æ—Ü–∏ –∑–∞ {target_symbol_id}.")
            return None

        # –ö–æ–Ω–≤–µ—Ä–∑–∏—ò–∞ –Ω–∞ Decimal128 –≤–æ float
        numeric_cols = ['open', 'high', 'low', 'close', 'totalVolume']
        for col in numeric_cols:
            df[col] = df[col].apply(lambda x: float(str(x.to_decimal())) if isinstance(x, Decimal128) else x)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞ DataFrame
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df.set_index('timestamp', inplace=True)
        df.rename(columns={'totalVolume': 'Volume'}, inplace=True)

        print(f"–£—Å–ø–µ—à–Ω–æ –≤—á–∏—Ç–∞–Ω–∏ {len(df)} —Ä–µ–¥–æ–≤–∏.")
        return df

    except Exception as e:
        print(f"–ù–∞—Å—Ç–∞–Ω–∞ –≥—Ä–µ—à–∫–∞ –ø—Ä–∏ –ø–æ–≤—Ä–∑—É–≤–∞—ö–µ —Å–æ MongoDB –∏–ª–∏ –≤—á–∏—Ç—É–≤–∞—ö–µ: {e}")
        return None


def create_sequences(data, lookback, close_price_index):
    """
    –ö—Ä–µ–∏—Ä–∞ —Å–µ–∫–≤–µ–Ω—Ü–∏ (X) –∏ —Ç–∞—Ä–≥–µ—Ç–∏ (Y) –∑–∞ —Ç—Ä–µ–Ω–∏—Ä–∞—ö–µ –Ω–∞ LSTM.
    """
    X, Y = [], []
    for i in range(lookback, len(data)):
        # X: –í–ª–µ–∑–Ω–∏—Ç–µ –ø–æ–¥–∞—Ç–æ—Ü–∏ –∑–∞ LOOKBACK_PERIOD
        X.append(data[i - lookback:i, :])
        # Y: –¶–µ–Ω–∞—Ç–∞ –Ω–∞ —Å–ª–µ–¥–Ω–∏–æ—Ç –¥–µ–Ω (–∑–∞—Ç–≤–æ—Ä–µ–Ω–∞ —Ü–µ–Ω–∞)
        Y.append(data[i, close_price_index])
    return np.array(X), np.array(Y)


# --------------------------------------------------------
# 3. –¢–†–ï–ù–ò–†–ê–ä–ï –ò –ó–ê–ß–£–í–£–í–ê–ä–ï –ù–ê –ú–û–î–ï–õ–û–¢
# --------------------------------------------------------

def train_and_save_model(symbol_id: str, model_save_path: str, lookback_period: int, close_price_index: int):
    """
    –à–∞ –∏–∑–≤—Ä—à—É–≤–∞ —Ü–µ–ª–∞—Ç–∞ –ª–æ–≥–∏–∫–∞ –∑–∞ —Ç—Ä–µ–Ω–∏—Ä–∞—ö–µ –Ω–∞ –º–æ–¥–µ–ª–æ—Ç, –≥–æ –∑–∞—á—É–≤—É–≤–∞ –∏ –≥–æ –≤—Ä–∞—ú–∞.
    """
    print(f"\nüß† –ü–æ—á–Ω—É–≤–∞–º —Ç—Ä–µ–Ω–∏—Ä–∞—ö–µ –Ω–∞ –º–æ–¥–µ–ª –∑–∞ {symbol_id}...")

    # 1. –í—á–∏—Ç—É–≤–∞—ö–µ –ø–æ–¥–∞—Ç–æ—Ü–∏
    data = load_data_from_mongo(symbol_id)
    if data is None or data.shape[0] < lookback_period + 1:
        print("–ì–†–ï–®–ö–ê: –ù–µ–¥–æ–≤–æ–ª–Ω–æ –ø–æ–¥–∞—Ç–æ—Ü–∏ –∑–∞ —Ç—Ä–µ–Ω–∏—Ä–∞—ö–µ.")
        return None

    # 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—ò–∞
    features = ['open', 'high', 'low', 'close', 'Volume']
    data_to_scale = data[features].values
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data_to_scale)

    # 3. –ö—Ä–µ–∏—Ä–∞—ö–µ —Å–µ–∫–≤–µ–Ω—Ü–∏ –∏ –ø–æ–¥–µ–ª–±–∞ (70% Train)
    X, Y = create_sequences(scaled_data, lookback_period, close_price_index)
    TRAIN_SIZE = int(len(X) * 0.7)
    X_train, y_train = X[:TRAIN_SIZE], Y[:TRAIN_SIZE]
    X_test, y_test = X[TRAIN_SIZE:], Y[TRAIN_SIZE:]

    # 4. –ì—Ä–∞–¥–µ—ö–µ –∏ —Ç—Ä–µ–Ω–∏—Ä–∞—ö–µ –Ω–∞ LSTM –º–æ–¥–µ–ª–æ—Ç
    n_features = X_train.shape[2]

    # Keras Sequential API
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=(lookback_period, n_features)))
    model.add(Dropout(0.2))
    model.add(LSTM(units=50, return_sequences=False))
    model.add(Dropout(0.2))
    model.add(Dense(units=1))
    model.compile(optimizer='adam', loss='mse')

    model.fit(
        X_train,
        y_train,
        epochs=25,
        batch_size=32,
        validation_data=(X_test, y_test),
        verbose=0  # –¢—Ä–µ–Ω–∏—Ä–∞—ö–µ—Ç–æ —Ä–∞–±–æ—Ç–∏ –≤–æ –ø–æ–∑–∞–¥–∏–Ω–∞ –∑–∞ –¥–∞ –Ω–µ –≥–æ –æ–ø—Ç–æ–≤–∞—Ä—É–≤–∞ FastAPI
    )
    print("‚úÖ –¢—Ä–µ–Ω–∏—Ä–∞—ö–µ—Ç–æ –∑–∞–≤—Ä—à–∏. –ó–∞—á—É–≤—É–≤–∞–º –º–æ–¥–µ–ª...")

    # 5. –ó–∞—á—É–≤–∞—ò –≥–æ –º–æ–¥–µ–ª–æ—Ç
    try:
        model.save(model_save_path)
        print(f"‚úÖ LSTM –ú–æ–¥–µ–ª–æ—Ç –µ —É—Å–ø–µ—à–Ω–æ –∑–∞—á—É–≤–∞–Ω –≤–æ: {model_save_path}")
        return model
    except Exception as e:
        print(f"–ì–†–ï–®–ö–ê –ø—Ä–∏ –∑–∞—á—É–≤—É–≤–∞—ö–µ –Ω–∞ –º–æ–¥–µ–ª–æ—Ç: {e}")
        return model


# --------------------------------------------------------
# 4. –ì–õ–û–ë–ê–õ–ù–û –í–ß–ò–¢–£–í–ê–ä–ï –ù–ê –ú–û–î–ï–õ–û–¢ (–°–ï –ò–ó–í–†–®–£–í–ê –ü–†–ò –°–¢–ê–†–¢–£–í–ê–ä–ï)
# --------------------------------------------------------
GLOBAL_MODEL = None

if not os.path.exists(MODEL_SAVE_PATH):
    print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª–æ—Ç '{MODEL_SAVE_PATH}' –Ω–µ –ø–æ—Å—Ç–æ–∏. –ü–æ—á–Ω—É–≤–∞–º –ø—Ä–≤–∏—á–Ω–æ —Ç—Ä–µ–Ω–∏—Ä–∞—ö–µ...")
    # –ê–∫–æ –º–æ–¥–µ–ª–æ—Ç –Ω–µ –ø–æ—Å—Ç–æ–∏, —Ç—Ä–µ–Ω–∏—Ä–∞—ò –≥–æ –∑–∞ –¥–µ—Ñ–æ–ª—Ç–Ω–∏–æ—Ç —Å–∏–º–±–æ–ª
    GLOBAL_MODEL = train_and_save_model(
        TARGET_SYMBOL_ID,
        MODEL_SAVE_PATH,
        LOOKBACK_PERIOD,
        CLOSE_PRICE_INDEX
    )
else:
    try:
        # –û–±–∏–¥–∏ —Å–µ –¥–∞ –≤—á–∏—Ç–∞—à –ø–æ—Å—Ç–æ–µ—á–∫–∏ –º–æ–¥–µ–ª
        GLOBAL_MODEL = load_model(MODEL_SAVE_PATH,custom_objects={'mse': 'mse'})
        print("‚úÖ LSTM –ú–æ–¥–µ–ª–æ—Ç –µ —É—Å–ø–µ—à–Ω–æ –≤—á–∏—Ç–∞–Ω.")
    except Exception as e:
        print(f"‚ùó –§–ê–¢–ê–õ–ù–ê –ì–†–ï–®–ö–ê: –ù–µ –º–æ–∂–∞–º –¥–∞ –≥–æ –≤—á–∏—Ç–∞–º –º–æ–¥–µ–ª–æ—Ç. –ì—Ä–µ—à–∫–∞: {e}")
        # –ê–∫–æ –≤—á–∏—Ç—É–≤–∞—ö–µ—Ç–æ –ø—Ä–æ–ø–∞–¥–Ω–µ, –º–æ–¥–µ–ª–æ—Ç –æ—Å—Ç–∞–Ω—É–≤–∞ None.

if GLOBAL_MODEL is None:
    print("‚ùå –°–µ—Ä–≤–∏—Å–æ—Ç –Ω–µ–º–∞ –¥–∞ –º–æ–∂–µ –¥–∞ –ø—Ä–µ–¥–≤–∏–¥—É–≤–∞ –±–∏–¥–µ—ò—ú–∏ –º–æ–¥–µ–ª–æ—Ç –Ω–µ –µ –¥–æ—Å—Ç–∞–ø–µ–Ω.")


# --------------------------------------------------------
# 5. –ì–õ–ê–í–ù–ê –§–£–ù–ö–¶–ò–à–ê –ó–ê –ü–†–ï–î–í–ò–î–£–í–ê–ä–ï (–ü–æ–≤–∏–∫–∞–Ω–∞ –æ–¥ main.py)
# --------------------------------------------------------

def predict_for_symbol(symbol_id: str) -> dict:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä–∞ –ø—Ä–µ–¥–≤–∏–¥—É–≤–∞—ö–µ –∑–∞ —Å–ª–µ–¥–Ω–∏–æ—Ç –¥–µ–Ω –∑–∞ –¥–∞–¥–µ–Ω —Å–∏–º–±–æ–ª ID.
    """
    if GLOBAL_MODEL is None:
        return {"error": "Model not loaded. Check training logs."}

    # 1. –í—á–∏—Ç—É–≤–∞—ö–µ –ø–æ–¥–∞—Ç–æ—Ü–∏
    data = load_data_from_mongo(symbol_id)
    if data is None or data.empty or data.shape[0] < LOOKBACK_PERIOD:
        return {"error": f"Data not found or insufficient data ({data.shape[0]} rows) for symbol: {symbol_id}"}

    # 2. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—ò–∞ (MinMaxScaler —Ç—Ä–µ–±–∞ –¥–∞ —Å–µ —Ç—Ä–µ–Ω–∏—Ä–∞ –Ω–∞ –Ω–æ–≤–∏—Ç–µ –ø–æ–¥–∞—Ç–æ—Ü–∏)
    features = ['open', 'high', 'low', 'close', 'Volume']
    data_to_scale = data[features].values
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(data_to_scale)

    # 3. –ö—Ä–µ–∏—Ä–∞—ö–µ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∞—Ç–∞ —Å–µ–∫–≤–µ–Ω—Ü–∞ (last_30_days)
    last_30_days = scaled_data[-LOOKBACK_PERIOD:]
    n_features = scaled_data.shape[1]
    last_30_days_reshaped = np.reshape(last_30_days, (1, LOOKBACK_PERIOD, n_features))

    # 4. –ü—Ä–µ–¥–≤–∏–¥—É–≤–∞—ö–µ
    next_day_prediction_scaled = GLOBAL_MODEL.predict(last_30_days_reshaped, verbose=0)

    # 5. –î–µ-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—ò–∞
    # –ö—Ä–µ–∏—Ä–∞–º–µ "–ª–∞–∂–Ω–∞" –º–∞—Ç—Ä–∏—Ü–∞ –∑–∞ –¥–∞ –≥–æ –∫–æ—Ä–∏—Å—Ç–∏–º–µ –∏–Ω–≤–µ—Ä–∑–Ω–∏–æ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º
    predicted_price_full = np.zeros(shape=(1, n_features))
    predicted_price_full[0, CLOSE_PRICE_INDEX] = next_day_prediction_scaled[0, 0]
    next_day_price = scaler.inverse_transform(predicted_price_full)[0, CLOSE_PRICE_INDEX]

    # 6. –í—Ä–∞—ú–∞—ö–µ –Ω–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–æ—Ç
    return {
        "symbol": symbol_id,
        "prediction": round(float(next_day_price), 4),
        "last_price": round(float(data['close'].iloc[-1]), 4)
    }